{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data:  (54000, 20)\n",
      "Shape of training label:  (54000, 1)\n",
      "Shape of validation data:  (6000, 20)\n",
      "Shape of validation label:  (6000, 1)\n",
      "Shape of testing data:  (30000, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:47<00:00, 140.92it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 773.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training loss: 496.99692265500454 Validation Loss: 491.41401847860595\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:43<00:00, 153.49it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 805.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training loss: 496.8191731623884 Validation Loss: 489.98960785376613\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:43<00:00, 156.43it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 804.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training loss: 496.7891314519286 Validation Loss: 489.71798531729377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:42<00:00, 157.69it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 803.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training loss: 496.66142277392265 Validation Loss: 490.84479215631814\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:42<00:00, 157.37it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 802.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training loss: 496.4644599405738 Validation Loss: 493.8040797572412\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:42<00:00, 157.76it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 802.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training loss: 496.52645042025864 Validation Loss: 490.3012034655755\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:42<00:00, 157.83it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 803.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training loss: 496.4174776585282 Validation Loss: 490.1898725743721\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:42<00:00, 157.56it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 786.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training loss: 496.4361565060961 Validation Loss: 490.03183438255655\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:42<00:00, 157.98it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 801.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training loss: 496.33740423091865 Validation Loss: 491.2352602114388\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:42<00:00, 157.97it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 807.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Training loss: 496.3190266201621 Validation Loss: 489.81559100881276\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:42<00:00, 158.31it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 792.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Training loss: 496.2491690893839 Validation Loss: 490.44950290099524\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:46<00:00, 146.31it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 823.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Training loss: 496.260588928245 Validation Loss: 490.4788760237497\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:51<00:00, 130.03it/s]\n",
      "100%|██████████| 750/750 [00:01<00:00, 671.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Training loss: 496.1752448659306 Validation Loss: 490.17095321694063\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:52<00:00, 127.63it/s]\n",
      "100%|██████████| 750/750 [00:01<00:00, 666.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Training loss: 496.15623463302353 Validation Loss: 489.99106588634606\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:51<00:00, 130.08it/s]\n",
      "100%|██████████| 750/750 [00:01<00:00, 582.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Training loss: 496.1244671096888 Validation Loss: 490.21608891991445\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:49<00:00, 136.38it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 809.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Training loss: 496.12265839307014 Validation Loss: 490.00262178295196\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:47<00:00, 142.19it/s]\n",
      "100%|██████████| 750/750 [00:01<00:00, 736.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Training loss: 496.0663264336274 Validation Loss: 489.70853665992405\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:48<00:00, 140.19it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 759.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Training loss: 496.0305781234148 Validation Loss: 489.63044333892356\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:47<00:00, 141.26it/s]\n",
      "100%|██████████| 750/750 [00:01<00:00, 688.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Training loss: 496.0476915577458 Validation Loss: 490.0203753230786\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:47<00:00, 142.63it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 769.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Training loss: 496.02105823641773 Validation Loss: 489.85641135949214\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:48<00:00, 139.95it/s]\n",
      "100%|██████████| 750/750 [00:01<00:00, 610.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Training loss: 495.9896647818956 Validation Loss: 490.3196053288015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:49<00:00, 137.48it/s]\n",
      "100%|██████████| 750/750 [00:01<00:00, 685.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Training loss: 495.9831014530747 Validation Loss: 489.95243228934993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:47<00:00, 141.41it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 775.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Training loss: 495.97444161843185 Validation Loss: 489.86329438149545\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:48<00:00, 140.57it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 758.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Training loss: 495.9736835743624 Validation Loss: 489.7756595764449\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:47<00:00, 141.65it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 778.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Training loss: 495.9651988123915 Validation Loss: 489.925191387675\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:47<00:00, 140.69it/s]\n",
      "100%|██████████| 750/750 [00:01<00:00, 741.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Training loss: 495.9655138445664 Validation Loss: 490.0497731736032\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:49<00:00, 137.55it/s]\n",
      "100%|██████████| 750/750 [00:01<00:00, 719.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Training loss: 495.9607440878918 Validation Loss: 489.92139380657284\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:48<00:00, 138.84it/s]\n",
      "100%|██████████| 750/750 [00:01<00:00, 691.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Training loss: 495.94946454417624 Validation Loss: 489.9943959075791\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:49<00:00, 136.24it/s]\n",
      "100%|██████████| 750/750 [00:01<00:00, 720.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Training loss: 495.93639675890734 Validation Loss: 489.95144265677834\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [00:48<00:00, 138.55it/s]\n",
      "100%|██████████| 750/750 [00:00<00:00, 775.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Training loss: 495.9349005671508 Validation Loss: 490.0982911328647\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:03<00:00, 1203.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of final predictions is:  3750\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from shapely.geometry import Point\n",
    "from sklearn.neighbors import BallTree\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "\n",
    "# Suppress the FutureWarning related to is_categorical_dtype from TargetEncoder\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "class MyTargetEncoder():\n",
    "    def __init__(self, columns_to_target_encode, training_data):\n",
    "        self.encoders = {}\n",
    "        self.columns_to_target_encode = columns_to_target_encode\n",
    "        for col in columns_to_target_encode:\n",
    "            encoder = TargetEncoder()\n",
    "            encoder.fit(training_data[col], training_data['monthly_rent'])\n",
    "            self.encoders[col] = encoder\n",
    "        \n",
    "    def fit_data(self, encoded_data):\n",
    "        for col, encoder in self.encoders.items():\n",
    "            encoded_data[col] = encoder.transform(encoded_data[col])\n",
    "        return encoded_data\n",
    "\n",
    "\n",
    "def clean_data(data):\n",
    "    cleaned_data = data\n",
    "    # cleaned_data = cleaned_data.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "    cleaned_data = cleaned_data.drop(columns=['furnished', 'elevation', 'town', 'block', 'street_name', 'planning_area', 'subzone'])\n",
    "    cleaned_data['flat_type'] = cleaned_data['flat_type'].str.replace(r'(2|3|4|5)-room|(\\d) room', r'\\1\\2', regex=True)\n",
    "    cleaned_data['flat_type'] = cleaned_data['flat_type'].str.replace('executive', '6')\n",
    "    cleaned_data['flat_type'] = cleaned_data['flat_type'].astype(int)\n",
    "    cleaned_data['rent_approval_date'] = cleaned_data['rent_approval_date'].str[2:].str.replace('-', '', regex=False)\n",
    "    cleaned_data['rent_approval_date'] = cleaned_data['rent_approval_date'].astype(int)\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "def encode_data(train_org, training_cleaned, valid_cleaned, testing_cleaned):\n",
    "    # First Target Encoding\n",
    "    \n",
    "    columns_to_target_encode = ['flat_model']\n",
    "    myTargetEncoder = MyTargetEncoder(columns_to_target_encode, train_org)\n",
    "    \n",
    "    training_encoded = myTargetEncoder.fit_data(training_cleaned)\n",
    "    valid_encoded = myTargetEncoder.fit_data(valid_cleaned)\n",
    "    testing_encoded = myTargetEncoder.fit_data(testing_cleaned)\n",
    "    \n",
    "    # Now, One-Hot Encoding\n",
    "    \n",
    "    # Prepare Model\n",
    "    myOneHotEncoder = OneHotEncoder(sparse=False)\n",
    "    myOneHotEncoder.fit(training_encoded[['region']])\n",
    "    \n",
    "    # Fit on train data\n",
    "    tr1 = myOneHotEncoder.transform(training_encoded[['region']])\n",
    "    tr2 = pd.DataFrame(tr1, columns=myOneHotEncoder.get_feature_names_out(['region']))\n",
    "    tr3 = pd.concat([training_encoded.reset_index(drop=True), tr2.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    training_encoded = tr3.drop(columns=[\"region\"])\n",
    "    \n",
    "    # Fit on valid data\n",
    "    va1 = myOneHotEncoder.transform(valid_encoded[['region']])\n",
    "    va2 = pd.DataFrame(va1, columns=myOneHotEncoder.get_feature_names_out(['region']))\n",
    "    va3 = pd.concat([valid_encoded.reset_index(drop=True), va2.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    valid_encoded = va3.drop(columns=[\"region\"])\n",
    "    \n",
    "    # Fit on test data\n",
    "    te1 = myOneHotEncoder.transform(testing_encoded[['region']])\n",
    "    te2 = pd.DataFrame(te1, columns=myOneHotEncoder.get_feature_names_out(['region']))\n",
    "    te3 = pd.concat([testing_encoded.reset_index(drop=True), te2.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    testing_encoded = te3.drop(columns=[\"region\"])\n",
    "    \n",
    "    return training_encoded, valid_encoded, testing_encoded\n",
    "\n",
    "\n",
    "def scale_data(training_encoded, validation_encoded, testing_encoded):\n",
    "    scaler = StandardScaler()\n",
    "    training_scaled = scaler.fit_transform(training_encoded)\n",
    "    validation_scaled = scaler.fit_transform(validation_encoded)\n",
    "    testing_scaled = scaler.fit_transform(testing_encoded)\n",
    "    return training_scaled, validation_scaled, testing_scaled\n",
    "\n",
    "def preprocess_data(train_org, training_data_raw, valid_data_raw, testing_data_raw):\n",
    "    \n",
    "    training_cleaned = clean_data(training_data_raw)\n",
    "    valid_cleaned = clean_data(valid_data_raw)\n",
    "    testing_cleaned = clean_data(testing_data_raw)\n",
    "    \n",
    "    training_encoded, valid_encoded, testing_encoded = encode_data(train_org, training_cleaned, valid_cleaned, testing_cleaned)\n",
    "\n",
    "    return training_encoded, valid_encoded, testing_encoded\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_train, y_train):\n",
    "        self.x = torch.tensor(x_train, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.x[idx]\n",
    "        label = self.y[idx]\n",
    "\n",
    "        return features, label\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_data):\n",
    "        self.data = test_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        features = torch.FloatTensor(sample)  # Assuming the features are in a list or NumPy array\n",
    "        return features\n",
    "\n",
    "def get_data_loaders(X_train, y_train, X_val, y_val, X_test, batch_size):\n",
    "    train_dataset = CustomDataset(X_train, y_train.to_numpy())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    valid_dataset = CustomDataset(X_val, y_val.to_numpy())\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    test_dataset = TestDataset(X_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(32,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def add_aux_data_count_in_radius(training_data_raw, training_coords, col_name, aux_data_raw, radius):\n",
    "    geom_list_aux = [Point(lon,lat) for lon,lat in zip(aux_data_raw[\"longitude\"], aux_data_raw[\"latitude\"])]\n",
    "    gdf_aux = gpd.GeoDataFrame(aux_data_raw, geometry=geom_list_aux, crs=\"EPSG:4326\")\n",
    "\n",
    "    # this uses the right projection to get the distance in m scale\n",
    "    gdf_aux.to_crs(epsg=3414, inplace=True)\n",
    "    aux_coords = np.array(gdf_aux.geometry.apply(lambda point: (point.x, point.y)).tolist())\n",
    "\n",
    "    aux_tree = BallTree(aux_coords, leaf_size=20)\n",
    "    \n",
    "    # Perform the query\n",
    "    count_aux_within_radius = aux_tree.query_radius(training_coords, r=radius, count_only=True)\n",
    "    training_data_raw[col_name] = count_aux_within_radius\n",
    "\n",
    "    return training_data_raw\n",
    "\n",
    "def add_aux_data_nearest_dist(training_data_raw, training_coords, col_name, aux_data_raw):\n",
    "    geom_list_aux = [Point(lon,lat) for lon,lat in zip(aux_data_raw[\"longitude\"], aux_data_raw[\"latitude\"])]\n",
    "    gdf_aux = gpd.GeoDataFrame(aux_data_raw, geometry=geom_list_aux, crs=\"EPSG:4326\")\n",
    "\n",
    "    # this uses the right projection to get the distance in m scale\n",
    "    gdf_aux.to_crs(epsg=3414, inplace=True)\n",
    "    aux_coords = np.array(gdf_aux.geometry.apply(lambda point: (point.x, point.y)).tolist())\n",
    "\n",
    "    aux_tree = BallTree(aux_coords, leaf_size=20)\n",
    "\n",
    "    aux_distances, _ = aux_tree.query(training_coords, k=1)  # k=1 for finding the nearest point\n",
    "    training_data_raw[col_name] = aux_distances\n",
    "\n",
    "    return training_data_raw\n",
    "\n",
    "def add_aux_data(org_dataset):\n",
    "    # Add auxiliary data\n",
    "    df_schools = pd.read_csv('../auxiliary-data/sg-primary-schools.csv')\n",
    "    gep_schools = [\"Anglo-Chinese School (Primary)\", \"Catholic High School (Primary)\", \"Henry Park Primary School\",\n",
    "              \"Nan Hua Primary School\", \"Nanyang Primary School\", \"Raffles Girls' Primary School\", \"Rosyth School\",\n",
    "              \"St. Hilda's Primary School\", \"Tao Nan School\"]\n",
    "    df_gep_schools = df_schools[df_schools[\"name\"].isin(gep_schools)]\n",
    "    df_malls = pd.read_csv('../auxiliary-data/sg-shopping-malls.csv')\n",
    "    df_mrts = pd.read_csv('../auxiliary-data/sg-mrt-existing-stations.csv')\n",
    "\n",
    "    # org_dataset is either raw training or raw test data\n",
    "    geom_list = [Point(lon,lat) for lon,lat in zip(org_dataset[\"longitude\"], org_dataset[\"latitude\"])]\n",
    "    gdf_data = gpd.GeoDataFrame(org_dataset, geometry=geom_list, crs=\"EPSG:4326\")\n",
    "    # this uses the right projection to get the distance in m scale\n",
    "    gdf_data.to_crs(epsg=3414, inplace=True)\n",
    "    coords = np.array(gdf_data.geometry.apply(lambda point: (point.x, point.y)).tolist())\n",
    "\n",
    "    org_dataset = add_aux_data_count_in_radius(org_dataset, coords,\n",
    "                                                'pri_schs_within_6km', df_schools, 6000)\n",
    "    org_dataset = add_aux_data_count_in_radius(org_dataset, coords,\n",
    "                                                'gep_schs_within_5km', df_gep_schools, 5000)\n",
    "    org_dataset = add_aux_data_count_in_radius(org_dataset, coords,\n",
    "                                                'malls_within_3km', df_malls, 3000)\n",
    "    org_dataset = add_aux_data_count_in_radius(org_dataset, coords,\n",
    "                                                'mrts_within_3km', df_mrts, 3000)\n",
    "\n",
    "    org_dataset = add_aux_data_nearest_dist(org_dataset, coords, 'nearest_distance_to_gep',\n",
    "                                                  df_gep_schools)\n",
    "    org_dataset = add_aux_data_nearest_dist(org_dataset, coords, 'nearest_distance_to_mall',\n",
    "                                                  df_malls)\n",
    "    org_dataset = add_aux_data_nearest_dist(org_dataset, coords, 'nearest_distance_to_mrt',\n",
    "                                                  df_mrts)\n",
    "    return org_dataset\n",
    "\n",
    "\n",
    "def get_stock_data(average_monthly_data ,stock_name, year, month):\n",
    "    return average_monthly_data.loc[(stock_name, year, month)]\n",
    "\n",
    "def chunk(nameslist):\n",
    "    for i in range(0, len(nameslist), 10):\n",
    "        yield nameslist[i:i+10]\n",
    "\n",
    "def normalize(group):\n",
    "    min_val = group.min()\n",
    "    max_val = group.max()\n",
    "    group = (group - min_val) / (max_val - min_val)\n",
    "    return group\n",
    "\n",
    "def add_stock_data(org_dataset, is_test=False):\n",
    "    stockdata = pd.read_csv(\"../auxiliary-data/sg-stock-prices.csv\")\n",
    "\n",
    "    stockdata['date'] = pd.to_datetime(stockdata['date'])\n",
    "    stockdata['year'], stockdata['month'] = stockdata['date'].dt.year, stockdata['date'].dt.month\n",
    "    average_monthly_data = stockdata.groupby(['name', 'year', 'month']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    names = list(set(stockdata['name']))\n",
    "\n",
    "    average_monthly_data['normalized_value'] = average_monthly_data.groupby('name')['adjusted_close'].transform(normalize)\n",
    "\n",
    "    stockdata_pivot = average_monthly_data.pivot_table(index=['year', 'month'], columns='name', values='adjusted_close').reset_index()\n",
    "    stockdata_pivot['year'] = stockdata_pivot['year'].astype(int)\n",
    "    stockdata_pivot['month'] = stockdata_pivot['month'].astype(int)\n",
    "\n",
    "    org_dataset[['year', 'month']] = org_dataset['rent_approval_date'].str.split('-', expand=True)\n",
    "    org_dataset['year'] = org_dataset['year'].astype(int)\n",
    "    org_dataset['month'] = org_dataset['month'].astype(int)\n",
    "\n",
    "    merged = pd.merge(org_dataset, stockdata_pivot, on=['year', 'month'], how='left')\n",
    "\n",
    "    for stock in average_monthly_data['name'].unique():\n",
    "        merged[stock] = merged[stock].interpolate(method='nearest').ffill().bfill()\n",
    " \n",
    "    pos_corr_stocks = ['Keppel',\n",
    "    'Flex',\n",
    "    'Jardine Cycle & Carriage',\n",
    "    'Singapore Airlines',\n",
    "    'Golden Agri-Resources',\n",
    "    'OCBC Bank',\n",
    "    'Genting Singapore',\n",
    "    'DBS Group',\n",
    "    'Singtel',\n",
    "    'Sembcorp',\n",
    "    'UOB']\n",
    "\n",
    "    neg_corr_stocks = ['Great Eastern',\n",
    "    'SATS',\n",
    "    'Sea (Garena)',\n",
    "    'Mapletree Industrial Trust',\n",
    "    'Mapletree Commercial Trust',\n",
    "    'Singapore Post',\n",
    "    'Grab Holdings',\n",
    "    'Yanlord',\n",
    "    'Singapore Land',\n",
    "    'Karooooo',\n",
    "    'Riverstone Holdings',\n",
    "    'ComfortDelGro',\n",
    "    'IGG Inc',\n",
    "    'Triterras',\n",
    "    'Keppel REIT',\n",
    "    'ASLAN Pharmaceuticals']\n",
    "\n",
    "    merged['highest_pos_corr'] = merged[pos_corr_stocks].mean(axis=1)\n",
    "    merged = merged.drop(names, axis=1)\n",
    "    merged = merged.drop(['year', 'month'], axis=1)\n",
    "    return merged\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    training_data_raw = pd.read_csv('../train.csv')\n",
    "    testing_data_raw = pd.read_csv('../test.csv')\n",
    "    \n",
    "    training_data_raw = add_aux_data(training_data_raw)\n",
    "    testing_data_raw = add_aux_data(testing_data_raw)\n",
    "    \n",
    "    training_data_raw = add_stock_data(training_data_raw)\n",
    "    testing_data_raw = add_stock_data(testing_data_raw)\n",
    "    \n",
    "    train_X, train_y = training_data_raw.drop('monthly_rent', axis=1), training_data_raw[['monthly_rent']]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.1, random_state=42)\n",
    "    X_train, X_val, testing_data = preprocess_data(training_data_raw, X_train, X_val, testing_data_raw)\n",
    "\n",
    "    \n",
    "    X_train, X_val, X_test = scale_data(X_train, X_val, testing_data)\n",
    "    \n",
    "    print(\"Shape of training data: \", X_train.shape)\n",
    "    print(\"Shape of training label: \", y_train.shape)\n",
    "    print(\"Shape of validation data: \", X_val.shape)\n",
    "    print(\"Shape of validation label: \", y_val.shape)\n",
    "    print(\"Shape of testing data: \", X_test.shape)\n",
    "        \n",
    "    train_loader, val_loader, test_loader = get_data_loaders(X_train, y_train, X_val, y_val, X_test, batch_size=8)\n",
    "        \n",
    "    input_size = X_train.shape[1]\n",
    "    \n",
    "    device = \"mps\"\n",
    "    model = RegressionModel(input_size)\n",
    "    model.load_state_dict(torch.load(\"pretrained_weights.pth\"))\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer=optimizer,\n",
    "        step_size=1,\n",
    "        gamma=0.9,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    max_epochs = 30\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    min_loss = float(\"inf\")\n",
    "    \n",
    "    for e in range(max_epochs):\n",
    "        \n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for inp, label in tqdm(train_loader):\n",
    "            inp = inp.to(device)\n",
    "            label = label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(inp)\n",
    "            loss = criterion(y_pred, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    " \n",
    "        epoch_train_loss = np.mean(losses)\n",
    "        \n",
    "        # VALIDATING\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for inp, label in tqdm(val_loader):\n",
    "                inp = inp.to(device)\n",
    "                label = label.to(device)\n",
    "                y_pred = model(inp)\n",
    "                loss = criterion(y_pred, label)\n",
    "\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        epoch_valid_loss = np.mean(losses)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(\"Epoch {}: Training loss: {} Validation Loss: {}\\n\".format(e+1, math.sqrt(epoch_train_loss), math.sqrt(epoch_valid_loss)))\n",
    "        \n",
    "        if epoch_valid_loss < min_loss:\n",
    "            min_loss = epoch_valid_loss\n",
    "            torch.save(model.state_dict(), \"final_weights.pth\")\n",
    "        \n",
    "    \n",
    "    fin_model = RegressionModel(X_test.shape[1])\n",
    "    fin_model.to(device)\n",
    "    fin_model.load_state_dict(torch.load(\"final_weights.pth\"))\n",
    "    \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    final_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inp in tqdm(test_loader):\n",
    "            inp = inp.to(device)\n",
    "            y_pred = model(inp)\n",
    "            final_pred.append(y_pred)\n",
    "\n",
    "    print(\"Length of final predictions is: \", len(final_pred))\n",
    "    combined_tensor = torch.cat(final_pred, dim=0)\n",
    "    numpy_array = combined_tensor.cpu().numpy()\n",
    "    flattened_array = numpy_array.flatten()\n",
    "    ids = np.arange(30000)\n",
    "    df = pd.DataFrame({'Id': ids, 'Predicted': flattened_array})\n",
    "\n",
    "    df.to_csv(\"submission_neural_net.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
